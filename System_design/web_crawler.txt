A web crawler (also known as a spider or bot) is an automated program that systematically browses the World Wide Web. Its primary purpose is to visit web pages,
  read their content, and follow the links on those pages to discover new ones.

  Think of it as a digital librarian who reads every book in a library, and every time a book mentions another book, they go and find that one too. The ultimate
  goal is to create a massive, searchable index of the web's content, which is the foundational data source for a search engine like Google.

  Now, let's apply the framework to this problem.

  ---

  Design a Web Crawler

  Step 1: Clarify Requirements & Scope

  First, we ask clarifying questions to understand the system's goals.

   * Functional Requirements (What it does):
       * Given a starting URL, it must download the page.
       * It must extract all links from the downloaded page.
       * It must add the newly discovered links to a list of URLs to visit.
       * It must store the content of the downloaded pages in a database.
       * The process should repeat continuously for new URLs.

   * Non-Functional Requirements (How well it does it):
       * Scalability: The system must be able to crawl billions of pages.
       * Robustness: It must handle network failures, bad HTML, and unresponsive servers.
       * Politeness: It must not overwhelm any single web server with too many requests in a short period. It should respect robots.txt.
       * Extensibility: The system should be designed so that other components (like a search indexer or a data analysis pipeline) can easily process the downloaded
         content.

Step 2: High-Level Architecture (The "Boxes and Arrows")

  Now we'll sketch out the main components and how they interact. At this stage, we don't worry about the fine details, just the overall flow.

  Here is a simple, high-level design:

   1. Seed URLs: We start with a list of initial URLs to crawl (e.g., ["https://www.wikipedia.org", "https://news.google.com"]).

   2. URL Frontier: This is the heart of our crawler. It's a sophisticated queue that manages all the URLs we need to visit. It will take our seed URLs as its first
      input.

   3. Crawler Workers (Fetchers): These are a fleet of servers whose only job is to:
       * Take a URL from the URL Frontier.
       * Fetch the HTML content of that URL.
       * Pass the downloaded content to the next component.

   4. Parser Workers: These servers take the raw HTML content from the Crawler Workers and:
       * Extract the actual text content.
       * Extract all the links (<a href="...">) from the page.

   5. Data Storage:
       * The extracted text content is sent to a Content Store, a massive database designed to hold the content of billions of pages.
       * The extracted links are sent back to the URL Frontier to be scheduled for crawling.

  Here's the flow in a simple diagram:

    1                +-----------------+
    2                |   Seed URLs     |
    3                +-------+---------+
    4                        |
    5                        v
    6 +----------------------|----------------------+
    7 |              +-------v---------+             |
    8 |              |   URL Frontier  |             |
    9 |              +-------+---------+             |
   10 |                      |                       |
   11 |         (URLs to crawl)                      |
   12 |                      |                       |
   13 |                      v                       |
   14 |            +---------+---------+             |
   15 |            |  Crawler Workers  |             |
   16 |            +---------+---------+             |
   17 |                      |                       |
   18 |        (HTML content)                        |
   19 |                      |                       |
   20 |                      v                       |
   21 |             +--------+--------+              |
   22 |             |  Parser Workers |              |
   23 |             +--------+--------+              |
   24 |                |           |                 |
   25 | (Parsed text)  |           | (Extracted links)
   26 |                v           |                 |
   27 |      +---------+-------+   |                 |
   28 |      |   Content Store |   +-----------------+
   29 |      +-----------------+                     |
   30 +----------------------------------------------+

  This design decouples the main tasks: fetching, parsing, and storing. This is crucial for scalability, as we can now scale each component independently. For
  example, if fetching pages is slow, we can just add more Crawler Workers.

  Before we deep-dive into each component, does this high-level flow make sense? Any immediate questions about the responsibilities of each box?

╭────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│  > a few questions: 1. what is sophisticated about url frontier? can it not just be a queue of messages, maybe a queueing service like AWS SQS or even kafka? 2.   │
│    Would a noSQL db make more sense here since we are storing documents and would need a lot of scaling given the amount of crawling?                              │
╰────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯

✦ Those are two excellent and critical questions. An interviewer would expect you to think about exactly these points. Let's break them down.

  1. What is "sophisticated" about the URL Frontier?

  You are right to question this. A simple FIFO (First-In, First-Out) queue like a basic SQS queue would fail quickly for two main reasons: politeness and
  prioritization.

  a) Politeness: If we have 1,000 crawler workers all pulling from a simple queue, they might all happen to get URLs from the same small website (e.g.,
  joes-blog.com). This would flood the website with requests, effectively launching a denial-of-service attack. The URL Frontier must prevent this. It needs to
  manage crawl rates on a per-domain basis.

  b) Prioritization: Not all pages are equal. We want to crawl the home page of a major news site more frequently than a 10-year-old forum post. We also want to
  prioritize pages that are linked from many other important pages (a high PageRank). A simple queue has no concept of priority.

  So, the "sophistication" of the URL Frontier comes from the logic it's built with:

   * Internal Queues: It internally maintains many smaller queues, often one for each domain.
   * Scheduler: When a worker asks for a URL, the scheduler doesn't just pop from the top. It looks across its internal domain queues and picks a URL based on rules
     like:
       * Has enough time passed since we last hit this domain? (Politeness)
       * Which of the available domains has the highest priority page waiting? (Prioritization)
   * Duplicate Elimination: It needs to check if a newly discovered URL has already been crawled or is already in a queue. Doing this with a simple queue is
     inefficient. The Frontier often uses a data structure like a Bloom Filter for a fast, memory-efficient check to see if a URL is likely already seen, followed by
     a check against a more definitive store (like a database) if needed.

  So, while you might use a service like Kafka or SQS as a building block, the URL Frontier is the larger system you build around it to handle these critical
  crawler-specific requirements.

  2. Would a NoSQL DB make more sense?

  Yes, absolutely. You've hit on the perfect choice for the Content Store. A relational (SQL) database would be a very poor fit here for several reasons:

   * Massive Scale: We need to store billions or trillions of pages. A relational database cannot scale horizontally to this level easily. A NoSQL database like
     Cassandra or Google Bigtable is designed for exactly this kind of petabyte-scale, distributed storage.
   * Schema Flexibility: Web pages are unstructured. Some have lots of metadata, some have none. A NoSQL document or wide-column store doesn't require a rigid
     schema. We can just store a document with the URL as the key and dump whatever content and metadata we have.
   * Write-Heavy Workload: The crawler is constantly writing new documents. Many NoSQL databases are optimized for very high write throughput, which is exactly what
     we need.
   * No Need for Joins: We are storing self-contained pages. We don't need the complex JOIN operations that relational databases excel at. Our primary query pattern
     is "get me the page for this URL".

  So, for our Content Store, a horizontally scalable NoSQL wide-column store is the ideal choice.
 Step 3: Deep Dive & Identifying Bottlenecks

  Let's examine the lifecycle of a single URL and the potential issues at each stage.

  1. The URL Frontier

   * Component: As discussed, this is a complex system managing politeness, priority, and de-duplication. It might have internal queues per domain and a scheduler.
   * Bottleneck/Challenge: The Frontier itself can become a bottleneck. Every single URL to be crawled and every link extracted goes through it.
       * Single Point of Failure: If the Frontier service goes down, the entire crawling operation stops.
       * Solution: The Frontier must be a distributed, highly available service itself. We would run multiple instances of the Frontier, and they would use a
         coordination service like ZooKeeper to manage tasks and state.
       * Storage: The list of URLs can become enormous. Storing it all in memory is not feasible.
       * Solution: The Frontier would need its own persistent storage, likely a NoSQL database optimized for fast reads and writes, to hold the URLs and their
         priorities/schedules.

  2. The Crawler/Fetcher Workers

   * Component: A large set of simple machines that just fetch URLs.
   * Bottleneck/Challenge: Network I/O. These workers spend most of their time waiting for responses from web servers.
       * DNS Resolution: Looking up the IP address for every URL can be slow.
       * Solution: The workers should have a local DNS cache to store recent lookups. We could even have a dedicated internal DNS service for the crawler fleet.
       * "Spider Traps": A malicious or poorly designed website can generate an infinite number of links (e.g., a calendar with a "next month" link that goes on
         forever).
       * Solution: The URL Frontier must track the depth of a crawl and the number of URLs seen from a single domain to detect and de-prioritize potential traps.
       * Robustness: The workers must be resilient to timeouts, network errors, and various server responses (e.g., 404 Not Found, 503 Service Unavailable).
       * Solution: The code must have robust error handling. When a fetch fails, it should report the error back to the URL Frontier, which can then decide whether
         to retry later or discard the URL.

  3. The Parser Workers

   * Component: A fleet of servers that extract text and links from HTML.
   * Bottleneck/Challenge: CPU-intensive. Parsing malformed or complex HTML can consume a lot of CPU.
       * Solution: This is a classic horizontally scalable task. If parsing is slow, we simply add more Parser Worker machines. This is an "embarrassingly parallel"
         problem.

  4. The Content Store

   * Component: A massive NoSQL database.
   * Bottleneck/Challenge: Disk I/O and storage volume.
       * Storage Cost: Storing the raw HTML for billions of pages is expensive.
       * Solution: We should compress the HTML content before storing it. This provides a significant reduction in storage size at the cost of some CPU during writes
         and reads.
       * Hotspots: If we use the URL as the primary key, we might get "hotspots" if we crawl a single popular domain heavily, putting a lot of load on a single
         database shard.
       * Solution: The key should not be the raw URL, but a hash of the URL. This ensures a more uniform distribution of writes across all the database shards.

  5. Duplicate Content Detection

   * Challenge: Many URLs point to the exact same content. We don't want to store the same page multiple times.
   * Solution: As we parse a page, we can calculate a checksum (like a SHA-256 hash) of its text content. Before writing the content to our Content Store, we can
     check if we've already seen this checksum. If we have, we don't store the new content; we simply record that this new URL is another pointer to the existing
     content.

  This covers the main components and their challenges. We've designed a distributed, robust, and polite system.

  What part of this deep-dive would you like to explore further? For example, we could discuss the robots.txt protocol, different prioritization strategies, or how
  the final search index is built from this data.

╭────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│  > lets deep dive into the url frontier service and the content store. How would we implement the url frontier service? and if the idea is to have individual      │
│    queues per domain name, then that would mean too many queues to fetch from, for the crawler service. And why would we have individual queues oer domain name?   │
│    Are we going to crawl multiple paths on the same domain? for what reason? For database, what would be our sharding strategy?                                    │
╰────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯

✦ Excellent, let's get into the weeds. These are the kinds of implementation details that separate a basic design from a great one.

  ---

  Deep Dive: The URL Frontier

  Why have individual queues per domain?

  This is the cornerstone of politeness.

  Imagine we have 10,000 crawler workers. If they all pull from one big queue, it's statistically likely that for a short period, hundreds of them could grab URLs
  from cnn.com. This would hammer CNN's servers.

  By creating a logical queue for each domain (e.g., cnn.com, espn.com, my-blog.com), we can enforce rules at the domain level. The most important rule is a crawl
  delay. The Frontier's scheduler can be programmed to only release one URL from the cnn.com queue every, say, 2 seconds. This ensures we are a "good citizen" of
  the web.

  Are we going to crawl multiple paths on the same domain?

  Yes, absolutely. This is fundamental to how a crawler works. We start at the root (e.g., cnn.com), and on that page, we discover links to other pages within that
  same domain, like cnn.com/world, cnn.com/politics/article-123, etc. We must crawl these pages to discover the full content of the site and to find links to other
  external domains.

  How would we implement this? It sounds like too many queues!

  You're right, we wouldn't literally create millions of SQS queues or Kafka topics. This is a logical design that we would implement with a combination of data
  structures. Here’s a plausible implementation:

   1. Queueing Backend: We'd use a scalable message queue like Kafka. We wouldn't create a topic per domain. Instead, we might create a handful of topics based on
      priority. For example, a high-priority topic, a medium-priority topic, and a low-priority topic.

   2. The Scheduler Service: This is the "brains" of the Frontier. It's a separate microservice. Its job is to feed the crawler workers.
       * It maintains a data structure in memory (perhaps in a Redis cache for speed and persistence) that tracks the politeness constraints. This could be a hash
         map where:
           * Key: domain_name (e.g., "cnn.com")
           * Value: last_crawled_timestamp
       * The Scheduler reads messages from the high-level Kafka topics (e.g., from the high-priority topic first).
       * For each message (URL) it reads, it checks the domain against its politeness cache.
           * If now() - last_crawled_timestamp > crawl_delay, it's safe to crawl. The Scheduler forwards this URL to a simple "ready-to-crawl" queue that the workers
             consume from, and it updates the timestamp in its cache.
           * If it's not safe to crawl yet, the Scheduler doesn't forward the URL. It can either put it into a temporary "holding" state (e.g., a sorted set in Redis
             with the domain's next available crawl time as the score) or simply put it back into the Kafka topic to be re-evaluated later.

  This design gives us the best of both worlds: the scalability of Kafka for the raw storage of URLs, and the intelligence of a custom Scheduler service to enforce
  the critical politeness and prioritization rules without needing millions of queues.

  ---

  Deep Dive: The Content Store Sharding Strategy

  For the database, we've chosen a NoSQL wide-column store like Cassandra.

  Sharding Strategy: Consistent Hashing on the URL

  The most effective sharding strategy here is Consistent Hashing.

   1. The Key: As we discussed, we don't want to use the raw URL as the key because it can create hotspots. Instead, our primary key for a row will be a uniform hash
      of the URL (e.g., SHA-1 or MurmurHash). This hash value is our partition key.

   2. The Hash Ring: In Consistent Hashing, we imagine a circular ring representing all possible hash values. Each node (server) in our Cassandra cluster is assigned
      several random positions on this ring.

   3. Placement: To figure out where to store the data for a given URL, we calculate its hash and find that value on the ring. We then walk clockwise around the ring
      until we find the first server node. That's where the data will be stored. To ensure redundancy, the data is also replicated to the next N nodes on the ring
      (where N is our replication factor, usually 3).

  Why is this a good strategy?

   * Uniform Distribution: A good hash function will distribute the URLs evenly across the entire hash ring. This means all our server nodes will get a roughly equal
     amount of data and traffic, eliminating hotspots.
   * Minimal Rebalancing: When we add or remove a server from our cluster, only a small fraction of the keys need to be moved. For example, if we add a new node, it
     takes its place on the ring and only takes keys from its immediate neighbor, leaving the rest of the cluster untouched. This makes scaling the database up or
     down incredibly efficient.

  This combination of a smart URL Frontier and a consistently hashed Content Store gives us a robust and highly scalable backend for our crawler.

 ---

  1. Detailed Implementation of the Scheduler's "Holding State"

  The goal is to efficiently manage URLs that aren't ready to be crawled yet due to politeness rules, without losing them or constantly re-processing them. Putting
  them back into the main Kafka topic is simple, but inefficient. It creates a lot of repetitive work.

  The "holding state" using Redis is a much more elegant solution. Here’s how it works, using a Redis Sorted Set.

  A Redis Sorted Set is a data structure that holds a collection of unique strings (members) where each member is associated with a score (a floating-point
  number). The collection is kept sorted by this score.

  Implementation Steps:

   1. The Data Structure: We'll use a single Redis Sorted Set, let's call it wait_queue.
       * Member: The URL itself (e.g., "http://cnn.com/politics").
       * Score: The Unix timestamp of the earliest time this URL is allowed to be crawled. This is the last_crawled_timestamp + crawl_delay.

   2. The "Holding" Workflow:
       * Our Scheduler fetches a URL from the main Kafka topic (e.g., http://cnn.com/politics).
       * It checks its politeness cache and finds that cnn.com was just crawled 100ms ago, and the crawl delay is 2 seconds.
       * The Scheduler calculates the next_available_time = now() + 1.9 seconds.
       * It then executes a Redis command: ZADD wait_queue <next_available_time_as_timestamp> "http://cnn.com/politics".
       * This places the URL into the sorted set, ordered by when it should be re-evaluated.

   3. The "Retrieval" Workflow:
       * The Scheduler needs to check this wait_queue periodically to find URLs that have become available.
       * It can run a command every few milliseconds: ZRANGEBYSCORE wait_queue 0 <current_unix_timestamp>.
       * This command is extremely fast and efficient. It asks Redis: "Give me all the URLs whose score (their available time) is between 0 and now."
       * This instantly returns a list of all URLs that are now "unlocked" and ready to be crawled.
       * The Scheduler takes these URLs, removes them from the wait_queue (ZREM command), and forwards them to the "ready-to-crawl" queue for the workers.

  Why is this better?
  The Scheduler is no longer wasting time pulling URLs from Kafka that it can't use. It only interacts with URLs from the main topic when it has capacity, and it
  uses Redis's highly-optimized sorted sets to efficiently manage and retrieve the waiting URLs at the exact moment they become available.

  ---

  2. Deep Dive into Consistent Hashing

  Consistent Hashing is a strategy for distributing data across multiple servers (sharding) that minimizes data movement when servers are added or removed.

  The Problem with Simple Hashing

  Imagine you have 4 servers (N=4). A simple approach is to calculate hash(key) % N.
   * hash("apple") % 4 = 2. So "apple" goes to Server 2.
   * hash("banana") % 4 = 1. So "banana" goes to Server 1.

  What happens when you add a server? Now N=5.
   * hash("apple") % 5 = ???. It could be anything. It's very unlikely to be 2.
   * hash("banana") % 5 = ???. It's very unlikely to be 1.

  Almost all your existing keys now map to different servers! You have to remap and move nearly all your data. This is a catastrophic event for a large-scale
  system.

  The Consistent Hashing Solution

  Consistent Hashing solves this by using a "hash ring".

  Step 1: The Ring
  Imagine a circle representing a range of all possible hash values, from 0 to a very large number (e.g., 2^32 - 1).

   1       (0)
   2       / \
   3      /   \
   4 (2^32-1) (2^16)
   5      \   /
   6       \ /
   7     (2^24)

  Step 2: Place Servers on the Ring
  You take each of your servers (e.g., Server A, B, C) and hash their names or IP addresses to get a position on the ring.

   * hash("Server A") -> maps to position 1,000
   * hash("Server B") -> maps to position 15,000
   * hash("Server C") -> maps to position 25,000

   1       (A @ 1000)
   2       /       \
   3      /         \
   4 (C @ 25000)   (B @ 15000)
   5      \         /
   6       \       /
   7          ...

  Step 3: Place Data on the Ring
  Now, to store a piece of data (e.g., the URL http://example.com/page), you hash its key:

   * hash("http://example.com/page") -> maps to position 18,000

  You then find this position on the ring and "walk" clockwise until you find the first server. In this case, you'd hit Server C at position 25,000. So, the data
  for this URL is stored on Server C.

   * If a key hashes to 500, it goes to Server A.
   * If a key hashes to 12,000, it goes to Server B.
   * If a key hashes to 30,000, it wraps around the circle and goes to Server A.

  The Magic: Adding and Removing Servers

   * Adding a Server (Server D): Let's say we add Server D, and it hashes to position 17,000.

   1           (A @ 1000)
   2           /       \
   3          /         \
   4     (C @ 25000)   (B @ 15000)
   5          \         /
   6           \       /
   7         (D @ 17000)
      What data needs to move? Only the keys that live on Server C but whose hash is between 15,000 and 17,000. These keys, which previously mapped to C, now map to
  the new server D. No data on Server A or B is affected. We only moved a small, targeted fraction of the data.

   * Removing a Server (Server B): If Server B crashes, all the keys that were stored on it now simply continue walking clockwise and land on the next server, Server
     C. We only need to remap the data from the one failed server.

  This method ensures that when your cluster size changes, the amount of data you have to move is minimized, making your system far more stable and scalable.


